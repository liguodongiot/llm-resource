


**芯、寒武纪...


GPU市场占比，降序：浪潮、新华三、宁畅...


大模型训练：
- oam模组
- pcie


---


R5500-G6-- **芯、H20    oam模组

**芯 P800  350T算力 96GB显存   目前是 4 卡全互联，没法8卡全互联



R5330-G7-- 海光四号CPU


---

算力中心
异构算力、海量存储（分布式并行文件）、无损网络、主动安全


---


GPFS-NFS



多元算力统一纳管-天数智芯与NV混训-改动较大

天数与CUDA比较紧


多元GPU的 UCCL-统一通信库



算力 、显存  不同 

网络一致



方案中对NCCL、Pytorch、Megatron都进行过定制化改造。

目前该方案仅做过H800(PCIe 5.0)+天垓150(PCIe 4.0) + RoCE v2的方案。目前该方案还比较初级。不同算力、显存都不匹配，服务器内卡间通信也存在代差，模型并行切分策略需要纯手动切分，严重依赖于算法工程师对于不同加速卡的算力显存网络带宽的熟悉程度以及分布式训练框架的二次开发能力。目前仅有PCEI的方案，目前最佳是支持到4TP并行，超过4TP的模型并行网络瓶颈就很容易出现，对于超过50B参数规模的模型不推荐。






---
银行AI算力建设：


GPU卡：

国产卡：

GPU与国产卡并存：


---

算力资源池规划建议


生产区、开发测试区、预发布区、运维管理区、互联网接入区、广域网接入区、外联区


---


降序

--------智算业务区 、通用业务区

参数网

高速存储网

智算业务网

通算业务网



---

网络规划



参数网性能要求：
PCEI 4.0  主推100G、200G   PCIE 5.0  主推 200G/400G

存储网性能要求：主推100G、200G

IB / roce v2



---
存储规划


- 读场景  OPS
 
- 写场景  带宽占用    

- 容量占用  


综上： 建议配置100:1 的全闪高性能存储+100:3 的大容量混闪存储



华为、GPFS /DDM  高校?



-------


DCGM











----

roce 监控


IB 网卡  + ROCE交换机 + 天数智芯GPU



单机  23GB/s


多机 19GB/s

---


混训总结





h800 pcie  天hai 150



代码 （pytorch、通信库、metatron）


----

ROCE 
组网-网络负载均衡








参数网

存储网

业务网

带外管理网


-

单轨组网(昇腾) VS 多轨组网（nv）


-
H3C 推荐组网

单框/多框组网


盒盒组网（优）

框盒组网

DDC组网



--

二级盒盒组网



三级盒盒组网
1:3 收敛比   1：1收敛比


二级框盒组网



DDC组网




--

网络负载均衡


静态逐流调度--增强逐流调度--逐包调度--信元调度



解决负载均衡 ，调端、调网（流）-控制器



静态ECMP


路径导航


动态NSLB

LBN+昇腾-单轨-512卡（上线不高，下线不低）


DDC 方案  上线和下线一直很高，最大3000卡 ，成本高




FGLB



DLB Eligible Flow-Set 子流


动态逐包SprayLink



信元类


----

H3C 数据交换机芯片状态


商芯

国芯盛科


参数网芯片 存储网芯片


H3C  200G 400G 盒式 bps  双向  交换机


H3C  200G 400G 框式 
































