# llm-resourceï¼ˆLLM ç™¾å®ç®±ï¼‰

LLMå…¨æ ˆä¼˜è´¨èµ„æºæ±‡æ€»

> éå¸¸æ¬¢è¿å¤§å®¶ä¹Ÿå‚ä¸è¿›æ¥ï¼Œæ”¶é›†æ›´å¤šä¼˜è´¨å¤§æ¨¡å‹ç›¸å…³èµ„æºã€‚

## ç›®å½•

- ğŸ¼ [LLMç®—æ³•](#llmç®—æ³•)
- ğŸ˜ [LLMè®­ç»ƒ](#llmè®­ç»ƒ)
	- ğŸ˜ [LLMå¾®è°ƒ](#llmå¾®è°ƒ)
	- ğŸ¼ [LLMå¯¹é½](#llmå¯¹é½)
- ğŸ”¥ [LLMæ¨ç†](#llmæ¨ç†)
- :palm_tree: [LLMæ•°æ®å·¥ç¨‹ï¼ˆData Engineeringï¼‰](#llmæ•°æ®å·¥ç¨‹)
- ğŸ“¡ [LLMå‹ç¼©](#llmå‹ç¼©)
- ğŸ° [LLMæµ‹è¯„](#llmæµ‹è¯„)
- ğŸ˜ [AIåŸºç¡€çŸ¥è¯†](#aiåŸºç¡€çŸ¥è¯†)
- ğŸ“¡ [AIåŸºç¡€è®¾æ–½](#aiåŸºç¡€è®¾æ–½)
	- :palm_tree: [AIèŠ¯ç‰‡](#aièŠ¯ç‰‡)
	- ğŸ° [CUDA](#cuda)
- ğŸ˜ [AIç¼–è¯‘å™¨](#aiç¼–è¯‘å™¨)
- ğŸ° [AIæ¡†æ¶](#aiæ¡†æ¶)
- ğŸ“¡ [LLMåº”ç”¨å¼€å‘](#llmåº”ç”¨å¼€å‘)
- ğŸ˜ [LLMOps](#llmops)
- ğŸ“¡ [LLMå®è·µ](llmå®è·µ)
- ğŸ“¡[å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é›†é”¦](#å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é›†é”¦)



## LLMç®—æ³•


### Transformer

åŸç†ï¼š
- [Transformeræ¨¡å‹è¯¦è§£ï¼ˆå›¾è§£æœ€å®Œæ•´ç‰ˆ](https://zhuanlan.zhihu.com/p/338817680)
- [OpenAI ChatGPTï¼ˆä¸€ï¼‰ï¼šååˆ†é’Ÿè¯»æ‡‚ Transformer](https://zhuanlan.zhihu.com/p/600773858)
- [Transformerçš„ç»“æ„æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿå„ä¸ªå­æ¨¡å—å„æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ](https://blog.csdn.net/m0_54929869/article/details/118881804)
- [ä»¥Transformerç»“æ„ä¸ºåŸºç¡€çš„å¤§æ¨¡å‹å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ä»¥åŠKV cacheå‰–æ](https://mp.weixin.qq.com/s/3JYz6yrLeBr5ujip3LZe6w)
- [Transformer ä¸€èµ·åŠ¨æ‰‹ç¼–ç å­¦åŸç†](https://mp.weixin.qq.com/s/NgUNuWhvp2SqG-XWYv2PGQ)
- [ä¸ºä»€ä¹ˆtransformer(Bert)çš„å¤šå¤´æ³¨æ„åŠ›è¦å¯¹æ¯ä¸€ä¸ªheadè¿›è¡Œé™ç»´ï¼Ÿ](http://www.sniper97.cn/index.php/note/deep-learning/note-deep-learning/4002/)
- [Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)


æºç ï¼š

- [OpenAI ChatGPTï¼ˆä¸€ï¼‰ï¼šTensorflowå®ç°Transformer](https://zhuanlan.zhihu.com/p/603243890)
- [OpenAI ChatGPTï¼ˆä¸€ï¼‰ï¼šååˆ†é’Ÿè¯»æ‡‚ Transformer](https://zhuanlan.zhihu.com/p/600773858)
- [GPT ï¼ˆä¸€ï¼‰transformeråŸç†å’Œä»£ç è¯¦è§£](https://zhuanlan.zhihu.com/p/632880248)
- [Transformeræºç è¯¦è§£ï¼ˆPytorchç‰ˆæœ¬ï¼‰](https://zhuanlan.zhihu.com/p/398039366)
- [ææ‡‚Transformerç»“æ„ï¼Œçœ‹è¿™ç¯‡PyTorchå®ç°å°±å¤Ÿäº†](https://zhuanlan.zhihu.com/p/339207092)



### GPT1


### GPT2


- GPT2 æºç ï¼šhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py
- GPT2 æºç è§£æï¼šhttps://zhuanlan.zhihu.com/p/630970209
- nanoGPTï¼šhttps://github.com/karpathy/nanoGPT/blob/master/model.py


- 7.3 GPT2æ¨¡å‹æ·±åº¦è§£æï¼šhttp://121.199.45.168:13013/7_3.html
- GPTï¼ˆä¸‰ï¼‰GPT2åŸç†å’Œä»£ç è¯¦è§£: https://zhuanlan.zhihu.com/p/637782385
- GPT2å‚æ•°é‡å‰–æ: https://zhuanlan.zhihu.com/p/640501114


### ChatGPT

- [State of GPTï¼šå¤§ç¥Andrejæ­ç§˜OpenAIå¤§æ¨¡å‹åŸç†å’Œè®­ç»ƒè¿‡ç¨‹](https://mp.weixin.qq.com/s/zmEGzm1cdXupNoqZ65h7yg)
- [OpenAIè”åˆåˆ›å§‹äººäº²è‡ªä¸Šåœºç§‘æ™®GPTï¼Œè®©æŠ€æœ¯å°ç™½ä¹Ÿèƒ½ç†è§£æœ€å¼ºAI](https://mp.weixin.qq.com/s/MD4WwwJLXm8rEm-sniX8Gw)





### GLM

- [é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼šGLM](https://zhuanlan.zhihu.com/p/641499380)


### LLaMA



### MOE å¤§æ¨¡å‹

- [Mixtral-8x7B MoEå¤§æ¨¡å‹å¾®è°ƒå®è·µï¼Œè¶…è¶ŠLlama2-65B](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486657&idx=1&sn=c5a5e55b01243f477d063c9194d24f42&chksm=fd3be592ca4c6c84bf5eefff23dcc38eeb83624e9f53bbd9a72afba71e235dddf814549322ba&token=499509118&lang=zh_CN#rd)
- [å¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒå¹¶è¡ŒæŠ€æœ¯ï¼ˆå…«ï¼‰-MOEå¹¶è¡Œ](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486145&idx=1&sn=299c28153b286465be26e18153c6db5d&chksm=fd3be392ca4c6a84be283dad80f584443302ea29fc95744f83727e7d9d68952d3a0f8b1b66d5&token=499509118&lang=zh_CN#rd)
- [MoEæ¶æ„æ¨¡å‹çˆ†å‘æˆ–å°†å¸¦é£å›½äº§AIèŠ¯ç‰‡](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247488422&idx=1&sn=eeb18ec0f5b9e972df31d65e7db13f8f&chksm=fd3bfaf5ca4c73e38a696fe7b6f33a30af962fdddfabd92d74b1d06190442759aabe7b560f22&token=499509118&lang=zh_CN#rd)
- [å¤§æ¨¡å‹çš„æ¨¡å‹èåˆæ–¹æ³•æ¦‚è¿°](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247487652&idx=1&sn=1bbf692b6e1dc6bae719c8e0a10293a0&chksm=fd3bf9f7ca4c70e16473a98d5408f6daea5e8c116a88cb3f41dfb00ffb7f6016874ee092224c&token=499509118&lang=zh_CN#rd)
- [æ··åˆä¸“å®¶æ¨¡å‹ (MoE) è¯¦è§£](https://zhuanlan.zhihu.com/p/674698482)
- [ç¾¤é­”ä¹±èˆï¼šMoEå¤§æ¨¡å‹è¯¦è§£](https://zhuanlan.zhihu.com/p/677638939)
- [å¤§æ¨¡å‹LLMä¹‹æ··åˆä¸“å®¶æ¨¡å‹MoEï¼ˆä¸Š-åŸºç¡€ç¯‡ï¼‰](https://zhuanlan.zhihu.com/p/672712751)
- [å¤§æ¨¡å‹LLMä¹‹æ··åˆä¸“å®¶æ¨¡å‹MoEï¼ˆä¸‹-å®ç°ç¯‡ï¼‰](https://zhuanlan.zhihu.com/p/673048264)


### ä¸‹ä¸€ä»£å¤§æ¨¡å‹

- https://github.com/NExT-GPT/NExT-GPT
- https://next-gpt.github.io/
- [Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)



### å¤šæ¨¡æ€å¤§æ¨¡å‹

A Survey on Multimodal Large Language Modelsï¼šhttps://arxiv.org/pdf/2306.13549
Efficient-Multimodal-LLMs-Surveyï¼šhttps://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey


### å…¶ä»–

- [å¤§æ¨¡å‹æ—¶ä»£çš„å½’ä¸€åŒ–æŠ€æœ¯ï¼šè§£å¯†Transformeræ¶æ„ä¸­Pre-Normä¸RMSNormçš„é»„é‡‘ç»„åˆ](https://blog.csdn.net/qq_54445177/article/details/147096307)





## LLMè®­ç»ƒ


- [åˆ†å¸ƒå¼è®­ç»ƒ Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
- [OPT-175Bæ˜¯å¦‚ä½•ç‚¼æˆçš„](https://zhuanlan.zhihu.com/p/622061951)
- [å…¨ç½‘æœ€å…¨-æ··åˆç²¾åº¦è®­ç»ƒåŸç†](https://zhuanlan.zhihu.com/p/441591808)
- [é£æ¡¨åˆ†å¸ƒå¼è®­ç»ƒ4Dæ··åˆå¹¶è¡Œå¯è®­åƒäº¿çº§AIæ¨¡å‹](https://ai.baidu.com/forum/topic/show/987996)
- [Transformer Math 101](https://blog.eleuther.ai/transformer-math/) - å¦‚ä½•è®¡ç®—æ˜¾å­˜æ¶ˆè€—?
- [Megatron-LM ç¬¬ä¸‰ç¯‡Paperæ€»ç»“â€”â€”Sequence Parallelism & Selective Checkpointing](https://zhuanlan.zhihu.com/p/522198082)
- [å¤§æ¨¡å‹è®­ç»ƒè¸©å‘](https://zhuanlan.zhihu.com/p/660759033)


- å­¦ä¹ ç‡(warmup, decay)ï¼š
	- [æ¨¡å‹è°ƒä¼˜ï¼Œå­¦ä¹ ç‡è®¾ç½®ï¼ˆWarm Upã€lossè‡ªé€‚åº”è¡°å‡ç­‰ï¼‰ï¼Œbatch sizeè°ƒä¼˜æŠ€å·§ï¼ŒåŸºäºæ–¹å·®æ”¾ç¼©åˆå§‹åŒ–æ–¹æ³•](https://blog.csdn.net/sinat_39620217/article/details/130236886)
	- [æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå°æŠ€å·§](https://blog.csdn.net/sgyuanshi/article/details/108394444)


### LLMå¾®è°ƒ

- [Adapting P-Tuning to Solve Non-English Downstream Tasks](https://developer.nvidia.com/blog/adapting-p-tuning-to-solve-non-english-downstream-tasks/)


### LLMå¯¹é½

- [MOSS-RLHF](https://github.com/OpenLMLab/MOSS-RLHF)
- [æ¨¡å‹è°ƒä¼˜ï¼ˆRLHF/DPO/ORPOï¼‰- ç»ˆææŒ‡å—](https://zhuanlan.zhihu.com/p/692594519)
- [DPO: Direct Preference Optimization è®ºæ–‡è§£è¯»åŠä»£ç å®è·µ](https://zhuanlan.zhihu.com/p/642569664)
- [å¼ºåŒ–å­¦ä¹ å…¥é—¨ï¼šåŸºæœ¬æ€æƒ³å’Œç»å…¸ç®—æ³•](https://imzhanghao.com/2022/02/10/reinforcement-learning/)
- [äººäººéƒ½èƒ½çœ‹æ‡‚çš„PPOåŸç†ä¸æºç è§£è¯»](https://zhuanlan.zhihu.com/p/677607581)
- [å…³äºInstruct GPTå¤ç°çš„ä¸€äº›ç»†èŠ‚ä¸æƒ³æ³•](https://zhuanlan.zhihu.com/p/609078527)
- [ã€RLHFã€‘RL ç©¶ç«Ÿæ˜¯å¦‚ä½•ä¸ LLM åšç»“åˆçš„ï¼Ÿ](https://zhuanlan.zhihu.com/p/675329917)
- [ã€RLHFã€‘æƒ³è®­ç»ƒChatGPTï¼Ÿå¾—å…ˆå¼„æ˜ç™½Reward Modelæ€ä¹ˆè®­ï¼ˆé™„æºç ï¼‰](https://zhuanlan.zhihu.com/p/595579042)
- [Reinforcement Learning from Human Feedback å…¨å®¶æ¡¶ï¼ˆRL ä¾§ï¼‰](https://zhuanlan.zhihu.com/p/700149886)

paper:

- [LLMå¯¹é½ç»¼è¿°](https://arxiv.org/pdf/2407.16216)
- [RLHF-PPO](https://arxiv.org/pdf/2203.02155)
- [DPO](https://arxiv.org/pdf/2305.18290)
- [ORPO](https://arxiv.org/pdf/2403.07691)


## LLMæ¨ç†


- [ä½¿ç”¨HuggingFaceçš„Accelerateåº“åŠ è½½å’Œè¿è¡Œè¶…å¤§æ¨¡å‹](https://zhuanlan.zhihu.com/p/605640431) : device_mapã€no_split_module_classesã€ offload_folderã€ offload_state_dict
- [å€ŸåŠ© PyTorchï¼ŒAccelerate å¦‚ä½•è¿è¡Œè¶…å¤§æ¨¡å‹](https://huggingface.co/blog/accelerate-large-models)
- [ä½¿ç”¨ DeepSpeed å’Œ Accelerate è¿›è¡Œè¶…å¿« BLOOM æ¨¡å‹æ¨ç†](https://huggingface.co/blog/zh/bloom-inference-pytorch-scripts)
- [LLMä¸ƒç§æ¨ç†æœåŠ¡æ¡†æ¶æ€»ç»“](https://zhuanlan.zhihu.com/p/653352979)
- [LLMæŠ•æœºé‡‡æ ·ï¼ˆSpeculative Samplingï¼‰ä¸ºä½•èƒ½åŠ é€Ÿæ¨¡å‹æ¨ç†](https://zhuanlan.zhihu.com/p/653734659)
- [å¤§æ¨¡å‹æ¨ç†å¦™æ‹›â€”æŠ•æœºé‡‡æ ·ï¼ˆSpeculative Decodingï¼‰](https://zhuanlan.zhihu.com/p/651359908)
- https://github.com/flexflow/FlexFlow/tree/inference
- [TensorRT-LLM(3)--æ¶æ„](https://zhuanlan.zhihu.com/p/665595557)
- NLPï¼ˆåå…«ï¼‰ï¼šLLM çš„æ¨ç†ä¼˜åŒ–æŠ€æœ¯çºµè§ˆï¼šhttps://zhuanlan.zhihu.com/p/642412124
- â€‹æ­ç§˜NVIDIAå¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼šTensorRT-LLMï¼šhttps://zhuanlan.zhihu.com/p/680808866
- [å¦‚ä½•ç”Ÿæˆæ–‡æœ¬: é€šè¿‡ Transformers ç”¨ä¸åŒçš„è§£ç æ–¹æ³•ç”Ÿæˆæ–‡æœ¬](https://huggingface.co/blog/zh/how-to-generate) | [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)
- [DeepSeek-V3 / R1 æ¨ç†ç³»ç»Ÿæ¦‚è§ˆ](https://zhuanlan.zhihu.com/p/27181462601)



### å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯


KV Cacheï¼š
- [å›¾è§£å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–ï¼šKV Cache](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486956&idx=1&sn=cd5e36857bbd8ebd750d2c172550d2bd&chksm=fd3be4bfca4c6da9f2276310995c7d60a42c0d01a960a42a38226cf954bab0d2d2a5772905df&token=1409805983&lang=zh_CN#rd)
- [å¤§æ¨¡å‹æ¨ç†ç™¾å€åŠ é€Ÿä¹‹KV cacheç¯‡](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247487886&idx=1&sn=38d3cd36c6c5acb2fe5c80ceffcba2cf&chksm=fd3bf8ddca4c71cb243566b593dfa095926b003a4a06442cc96e8ce3f2c64171b34f0bca8428&token=1409805983&lang=zh_CN#rd)
- [å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿï¼šçœ‹å›¾å­¦KV Cache](https://zhuanlan.zhihu.com/p/662498827)
- [å¤§æ¨¡å‹æ¨ç†æ€§èƒ½ä¼˜åŒ–ä¹‹KV Cacheè§£è¯»](https://zhuanlan.zhihu.com/p/630832593)


è§£ç ä¼˜åŒ–ï¼š
- [å¤§æ¨¡å‹æ¨ç†å¦™æ‹›â€”æŠ•æœºé‡‡æ ·ï¼ˆSpeculative Decodingï¼‰](https://zhuanlan.zhihu.com/p/651359908)




### vLLM

- [vLLMï¼ˆå…­ï¼‰æºç è§£è¯»ä¸‹ @HelloWorld](https://zhuanlan.zhihu.com/p/694442998)
- [çŒ›çŒ¿ï¼šå›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ï¼švLLMæºç è§£æ1ï¼Œæ•´ä½“æ¶æ„](https://zhuanlan.zhihu.com/p/691045737)
- [LLMæ¨ç†2ï¼švLLMæºç å­¦ä¹  @ akaihaoshuai ](https://zhuanlan.zhihu.com/p/643336063)
- [å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ vLLM æºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¡†æ¶æ¦‚è§ˆ](https://zhuanlan.zhihu.com/p/681402162)


## LLMæ•°æ®å·¥ç¨‹

- [An Initial Exploration of Theoretical Support for Language Model Data Engineering. Part 1: Pretraining @
ç¬¦å°§](https://yaofu.notion.site/An-Initial-Exploration-of-Theoretical-Support-for-Language-Model-Data-Engineering-Part-1-Pretraini-dc480d9bf7ff4659afd8c9fb738086eb)



## LLMå‹ç¼©



- [Awesome Model Quantization](https://github.com/htqin/awesome-model-quantization)
- [Efficient-LLMs-Survey](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)
- [Awesome LLM Compression](https://github.com/HuangOwen/Awesome-LLM-Compression)
- [æ¨¡å‹è½¬æ¢ã€æ¨¡å‹å‹ç¼©ã€æ¨¡å‹åŠ é€Ÿå·¥å…·æ±‡æ€»](https://blog.csdn.net/WZZ18191171661/article/details/99700992)
- [AI æ¡†æ¶éƒ¨ç½²æ–¹æ¡ˆä¹‹æ¨¡å‹è½¬æ¢](https://zhuanlan.zhihu.com/p/396781295)
- [Pytorch æ¨¡å‹è½¬ TensorRT (torch2trt æ•™ç¨‹)](https://zhuanlan.zhihu.com/p/570822430)



## LLMæµ‹è¯„

- [CLiBä¸­æ–‡å¤§æ¨¡å‹èƒ½åŠ›è¯„æµ‹æ¦œå•](https://github.com/jeinlee1991/chinese-llm-benchmark)
- [huggingface Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- HELMï¼šhttps://github.com/stanford-crfm/helm
- HELMï¼šhttps://crfm.stanford.edu/helm/latest/
- lm-evaluation-harnessï¼šhttps://github.com/EleutherAI/lm-evaluation-harness/
- CLEVAï¼šhttp://www.lavicleva.com/#/homepage/overview
- CLEVAï¼šhttps://github.com/LaVi-Lab/CLEVA/blob/main/README_zh-CN.md



## æç¤ºå·¥ç¨‹


- [åšæ•°æ®å…³é”®æ­¥éª¤ï¼šæ€ä¹ˆå†™å¥½promptï¼Ÿ](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486771&idx=1&sn=359c029b010d7ad96fff33952ad634a8&chksm=fd3be460ca4c6d76b4996f971ff21080ca0a83f3042893bb6827752ad8af812b4afeb1151af1&token=1288418017&lang=zh_CN#rd)
- [ä»1000+æ¨¡æ¿ä¸­æ€»ç»“å‡ºçš„10å¤§æç¤ºå·¥ç¨‹æ–¹æ³•åŠ©ä½ æˆä¸ºæç¤ºè¯å¤§å¸ˆï¼](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486174&idx=1&sn=97ddcd5fb44eb4e3143fa746b7d617c8&chksm=fd3be38dca4c6a9b94fb88bd3f7a5009dee53812412e6f62f9f0a5955d165dd0d5f6ce698208&scene=21#wechat_redirect)
- [ä¸€æ–‡ææ‡‚æç¤ºå·¥ç¨‹çš„åŸç†åŠå‰ä¸–ä»Šç”Ÿ](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247485231&idx=1&sn=acfa77264da611983a49297ab8376e8f&chksm=fd3bee7cca4c676a3ccbc459e70a9e9920b08369a4d618c4ed550c96e9acd09b594cc04b21a6&scene=21#wechat_redirect)
- [Effective Prompt: ç¼–å†™é«˜è´¨é‡Promptçš„14ä¸ªæœ‰æ•ˆæ–¹æ³•](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486087&idx=1&sn=118b82abd4b22975e9aeb9f23ed0c9c5&chksm=fd3be3d4ca4c6ac2b41f1c3e908b845d4497a84dc9741034d1e1a830cba93515439b60a835e5&scene=21#wechat_redirect)
- [æç¤ºå·¥ç¨‹å’Œæç¤ºæ„é€ æŠ€å·§](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247487107&idx=1&sn=337325ee6a9a4d4c56821b1e759f1555&chksm=fd3be7d0ca4c6ec60b6394bf76282ee3eef6beccfe2c31885cbb111a5bdc32022ba346509681&token=1288418017&lang=zh_CN#rd)
- [ä¸€æ–‡å¸¦ä½ äº†è§£æç¤ºæ”»å‡»ï¼](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247485936&idx=1&sn=0bcc72e5bfeb50c437253626d763f67d&chksm=fd3be0a3ca4c69b52bba0e0f22730b497c56fad99444b23d437cf49262cd5e52489fb141d338&token=1288418017&lang=zh_CN#rd)



## ç»¼åˆ

- [é€šå‘AGIä¹‹è·¯ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ç²¾è¦](https://zhuanlan.zhihu.com/p/597586623)
- [å¤§è¯­è¨€æ¨¡å‹çš„æ¶Œç°èƒ½åŠ›ï¼šç°è±¡ä¸è§£é‡Š](https://zhuanlan.zhihu.com/p/621438653)
- [NLPï¼ˆåå…«ï¼‰ï¼šLLM çš„æ¨ç†ä¼˜åŒ–æŠ€æœ¯çºµè§ˆ](https://zhuanlan.zhihu.com/p/642412124)
- [å¹¶è¡Œè®¡ç®—3ï¼šå¹¶è¡Œè®¡ç®—æ¨¡å‹](https://zhuanlan.zhihu.com/p/568947162)
- [å¤§æ¨¡å‹â€œå¹»è§‰â€ï¼Œçœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº† | å“ˆå·¥å¤§åä¸ºå‡ºå“](https://www.thepaper.cn/newsDetail_forward_25344873)
- [æ·±å…¥ç†è§£è¯­è¨€æ¨¡å‹çš„å›°æƒ‘åº¦(perplexity)](https://zhuanlan.zhihu.com/p/686808564)



**safetensors**ï¼š

- [binå’ŒsafetensorsåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ](https://www.zhihu.com/question/629624037/answer/3307818120)
- [Safetensorsï¼šä¿å­˜æ¨¡å‹æƒé‡çš„æ–°æ ¼å¼](https://zhuanlan.zhihu.com/p/691446249)
- [github: safetensors](https://github.com/huggingface/safetensors)
- [huggingface: safetensors](https://huggingface.co/docs/safetensors/index)
- [Safetensors: a simple, safe and faster way to store and distribute tensors.](https://medium.com/@mandalsouvik/safetensors-a-simple-and-safe-way-to-store-and-distribute-tensors-d9ba1931ba04)
- https://huggingface.co/docs/safetensors/index
- https://github.com/huggingface/safetensors/tree/v0.3.3
- [æ‰‹æŠŠæ‰‹æ•™ä½ ï¼šLLama2åŸå§‹æƒé‡è½¬HFæ¨¡å‹](https://zhuanlan.zhihu.com/p/669158180)


## AIæ¡†æ¶




### PyTorch

- [PyTorch æºç è§£è¯»ç³»åˆ—](https://zhuanlan.zhihu.com/p/328674159) @ OpenMMLab å›¢é˜Ÿ
- [[æºç è§£æ] PyTorch åˆ†å¸ƒå¼](https://juejin.cn/post/7026144707591815175) @ ç½—è¥¿çš„æ€è€ƒ
- [PyTorch åˆ†å¸ƒå¼(18) --- ä½¿ç”¨ RPC çš„åˆ†å¸ƒå¼æµæ°´çº¿å¹¶è¡Œ](https://juejin.cn/post/7043601075307282462) @ ç½—è¥¿çš„æ€è€ƒ
- [ã€Pytorchã€‘model.train() å’Œ model.eval() åŸç†ä¸ç”¨æ³•](https://blog.csdn.net/weixin_44211968/article/details/123774649)

### DeepSpeed

- [DeepSpeedä½¿ç”¨æŒ‡å—(ç®€ç•¥ç‰ˆ)](https://blog.csdn.net/weixin_43301333/article/details/127237122)
- [å…³äºDeepspeedçš„ä¸€äº›æ€»ç»“ä¸å¿ƒå¾—](https://zhuanlan.zhihu.com/p/650824387)


### Megatron-LM

- [Megatron-LM è¿‘æœŸçš„æ”¹åŠ¨](https://zhuanlan.zhihu.com/p/651192295)
- [æ·±å…¥ç†è§£ Megatron-LMï¼ˆ1ï¼‰åŸºç¡€çŸ¥è¯†](https://zhuanlan.zhihu.com/p/650234985) @ ç®€æ«
- [æ·±å…¥ç†è§£ Megatron-LMï¼ˆ2ï¼‰åŸç†ä»‹ç»](https://zhuanlan.zhihu.com/p/650383289)
- [[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (1) --- è®ºæ–‡ & åŸºç¡€](https://juejin.cn/post/7057837676430360584) @ ç½—è¥¿çš„æ€è€ƒ
- [[æºç è§£æ] æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒMegatron (2) --- æ•´ä½“æ¶æ„](https://juejin.cn/post/7061942798957674504)
- [[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(1)](https://zhuanlan.zhihu.com/p/366906920) @è¿·é€”å°ä¹¦åƒ®â€‹
- [[ç»†è¯»ç»å…¸]Megatronè®ºæ–‡å’Œä»£ç è¯¦ç»†åˆ†æ(2)](https://zhuanlan.zhihu.com/p/388830967)


### Megatron-DeepSpeed


### Huggingface Transformers




## [AIåŸºç¡€çŸ¥è¯†](./ai-base.md)


## AIåŸºç¡€è®¾æ–½

### AIèŠ¯ç‰‡

- [ä¸šç•ŒAIåŠ é€ŸèŠ¯ç‰‡æµ…æï¼ˆä¸€ï¼‰ç™¾åº¦æ˜†ä»‘èŠ¯](https://zhuanlan.zhihu.com/p/593143821)
- NVIDIA CUDA-X AIï¼šhttps://www.nvidia.cn/technologies/cuda-x/
- [Intelï¼ŒNvidiaï¼ŒAMDä¸‰å¤§å·¨å¤´ç«æ‹¼GPUä¸CPU](https://zhuanlan.zhihu.com/p/629024100)
- å¤„ç†å™¨ä¸AIèŠ¯ç‰‡-Google-TPUï¼šhttps://zhuanlan.zhihu.com/p/646793355
- [ä¸€æ–‡çœ‹æ‡‚å›½äº§AIèŠ¯ç‰‡ç©å®¶](https://www.xckfsq.com/news/show.html?id=29187)
- [æ·±åº¦ | å›½äº§AIèŠ¯ç‰‡ï¼Œç©å®¶å‡ ä½•](https://mp.weixin.qq.com/s?__biz=MzIwMzgzNTQ1Nw==&mid=2247599349&idx=1&sn=12459cbc418d3831d0c28e87ddb71b2f&scene=21#wechat_redirect)


### CUDA

- [CUDAç¼–ç¨‹å…¥é—¨ï¼ˆä¸€ï¼‰CUDAç¼–ç¨‹æ¨¡å‹](https://zhuanlan.zhihu.com/p/97044592)
- [CUDAç¼–ç¨‹å…¥é—¨ï¼ˆäºŒï¼‰GPUç¡¬ä»¶åŸºç¡€](https://zhuanlan.zhihu.com/p/97131966)
- [GPUç¼–ç¨‹ï¼ˆCUDAï¼‰](https://face2ai.com/program-blog/)
- [CUDAç¼–ç¨‹å…¥é—¨æç®€æ•™ç¨‹](https://zhuanlan.zhihu.com/p/34587739)




## AIç¼–è¯‘å™¨

- [TVMèµ„æ–™](https://github.com/BBuf/tvm_mlir_learn)
- [AIç¼–è¯‘å™¨åŸç†](https://www.bilibili.com/read/cv21242696/?spm_id_from=333.999.0.0) @ZIMOé…±


## LLMåº”ç”¨å¼€å‘

- [åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘](https://github.com/datawhalechina/llm-universe)
- [langchain java](https://github.com/HamaWhiteGG/langchain-java)
- [å¤§æ¨¡å‹ä¸»æµåº”ç”¨RAGçš„ä»‹ç»â€”â€”ä»æ¶æ„åˆ°æŠ€æœ¯ç»†èŠ‚](https://luxiangdong.com/2023/09/25/ragone/#/%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2)
- [åŸºäºæ£€ç´¢çš„å¤§è¯­è¨€æ¨¡å‹å’Œåº”ç”¨ï¼ˆé™ˆä¸¹ç¦ï¼‰](https://acl2023-retrieval-lm.github.io/)
- [å¤§æ¨¡å‹bad caseä¿®å¤æ–¹æ¡ˆæ€è€ƒ](https://mp.weixin.qq.com/s/xqFkfzHVnePf1ub_sCk9iw)
- [ã€Šç»¼è¿°ï¼šå…¨æ–°å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„Agentã€‹â€”â€”4.5ä¸‡å­—è¯¦ç»†è§£è¯»å¤æ—¦NLPå’Œç±³å“ˆæ¸¸æœ€æ–°Agent Survey](https://zhuanlan.zhihu.com/p/656676717)




## LLMOps

- [MLOps Landscape in 2023: Top Tools and Platforms](https://neptune.ai/blog/mlops-tools-platforms-landscape)
- [What Constitutes A Large Language Model Application?  ](https://cobusgreyling.medium.com/what-constitutes-a-large-language-model-application-bacf81103475)ï¼šLLM Functionality Landscape
- [AI System @åƒæœå†»ä¸åæœå†»çš®](https://github.com/liguodongiot/ai-system)




## RAG

- https://github.com/hymie122/RAG-Survey

## ä¹¦ç±

- å¤§è¯­è¨€æ¨¡å‹åŸç†ä¸å·¥ç¨‹ @æ¨é’
- [å¤§è¯­è¨€æ¨¡å‹ä»ç†è®ºåˆ°å®è·µ](https://intro-llm.github.io/chapter/LLM-TAP.pdf) @å¼ å¥‡ ï¼šhttps://intro-llm.github.io/
- [åŠ¨æ‰‹å­¦å¤§æ¨¡å‹](https://github.com/Lordog/dive-into-llms?tab=readme-ov-file)

## LLMå®è·µ

- [minGPT @karpathy](https://github.com/karpathy/minGPT)
- [llm.c @karpathy](https://github.com/karpathy/llm.c): LLM training in simple, raw C/CUDA
- [LLM101n](https://github.com/karpathy/LLM101n)
- [llama2.c](https://github.com/karpathy/llama2.c): Inference Llama 2 in one file of pure C
- [nanoGPT](https://github.com/karpathy/nanoGPT)
- [Baby-Llama2-Chinese](https://github.com/DLLXW/baby-llama2-chinese)
- [ä»0åˆ°1æ„å»ºä¸€ä¸ªMiniLLM](https://github.com/Tongjilibo/build_MiniLLM_from_scratch)
- [gpt-fast](https://github.com/pytorch-labs/gpt-fast) ã€[blog](https://pytorch.org/blog/accelerating-generative-ai-2/)
- [CSE 234: Data Systems for Machine Learning](https://hao-ai-lab.github.io/cse234-w25/)
- [DSC 291: Machine Learning Systems](https://hao-ai-lab.github.io/dsc291-s24/)


## å¤§æ¨¡å‹æ±‡æ€»èµ„æ–™

- [Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)
- [Awesome-LLM-Survey](https://github.com/HqWu-HITCS/Awesome-LLM-Survey)
- [Large Language Model Course](https://github.com/mlabonne/llm-course)
- [Awesome-Quantization-Papers](https://github.com/Zhen-Dong/Awesome-Quantization-Papers)
- [Awesome Model Quantization (GitHub)](https://github.com/htqin/awesome-model-quantization)
- [Awesome Transformer Attention (GitHub)](https://github.com/cmhungsteve/Awesome-Transformer-Attention)
- [è¯­è¨€æ¨¡å‹æ•°æ®é€‰æ‹©ç»¼è¿°](https://github.com/alon-albalak/data-selection-survey)
- [Awesome Knowledge Distillation of LLM Papers](https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs)
- [Awasome-Pruning @ghimiredhikura](https://github.com/ghimiredhikura/Awasome-Pruning)
- [Awesome-Pruning @he-y](https://github.com/he-y/Awesome-Pruning)
- [awesome-pruning @hrcheng1066](https://github.com/hrcheng1066/awesome-pruning)
- [Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)


## å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é›†é”¦

- [2024å¹´2æœˆå¤§æ¨¡å‹æ–‡ç« é›†é”¦](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247487320&idx=2&sn=522fdf838d4ec03f24dbc7a11a3a5a65&chksm=fd3be60bca4c6f1d0c9b0643db0d7334940fb592dac3b5fbf286c7232f6bb08b968fbd237a20&scene=21#wechat_redirect)
- [2024å¹´1æœˆå¤§æ¨¡å‹æ–‡ç« é›†é”¦](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247487067&idx=2&sn=33594e6a82cf79a7580272c064635d75&chksm=fd3be708ca4c6e1ece0e1f6cc22bfd286bf3e9073350b91369b1d0e7fb52b50fac8113288e43&scene=21#wechat_redirect)
- [2023å¹´12æœˆå¤§æ¨¡å‹æ–‡ç« é›†é”¦](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486824&idx=2&sn=4faaac42f983af46cce44b35dd416c5f&chksm=fd3be43bca4c6d2d6f5fd1cf3004c37782d0b829111ad5ecd155d6cd3adedd40655653271ba1&scene=21#wechat_redirect)
- [2023å¹´6-11æœˆå¤§æ¨¡å‹æ–‡ç« é›†é”¦](https://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&mid=2247486480&idx=2&sn=b6b504f9d67a3cdad5ba0eb68eee647b&chksm=fd3be543ca4c6c55e0c2fd335de92103a1aee4e5631be34f06d7557463bc7e339fb63680ad54&scene=21&poc_token=HCwA9WWjTC-CNeedW8iQ1lZwSAwg4fwWFAVcUnai)


## å…¶ä»–

- [Hugging Face åšå®¢](https://github.com/huggingface/blog/tree/main)



